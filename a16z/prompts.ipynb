{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt-1 (gpt o3 reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You are an advanced language model that has read and internalized the personality, reasoning, and thinking style expressed in the following blog posts:\n",
    "\n",
    "WEB-PAGES DATA:\n",
    "--------------------\n",
    "[\n",
    "    {\n",
    "        \"title\": \"Innovative Strategies in Tech\",\n",
    "        \"content\": [\"Technology is evolving rapidly...\", \"We believe in sustainable growth...\"]\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Leadership and Vision\",\n",
    "        \"content\": [\"A leader must inspire and innovate...\", \"The future is built on resilience...\"]\n",
    "    }\n",
    "]\n",
    "--------------------\n",
    "*Note: The web-pages data above is in JSON format (from \"crawled_data.json\") with each item containing a \"title\" and a list of paragraphs in \"content\".*\n",
    "\n",
    "Now, consider the following background information about our new product:\n",
    "\n",
    "CONTEXT:\n",
    "--------------------\n",
    "Our new product is an AI-powered tool that revolutionizes customer engagement by using advanced natural language processing techniques.\n",
    "--------------------\n",
    "*This context provides detailed background information about the product.*\n",
    "\n",
    "Finally, here is the user's query regarding the new product:\n",
    "\n",
    "USER QUERY:\n",
    "--------------------\n",
    "How can this product improve customer retention?\n",
    "--------------------\n",
    "\n",
    "Please answer the query using the reasoning, tone, and personality derived from the blog posts provided. Your answer should reflect the internalized thought patterns and style from the web-pages data while addressing the user's query in the context of the new product.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len. final_prompt: 4789\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load crawled data\n",
    "with open(\"blog_posts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    crawled_data = json.load(f)\n",
    "\n",
    "# Convert list of pages into structured text\n",
    "web_pages_data = \"\\n\\n\".join([f\"Title: {page['title']}\\nContent:\\n\" + \"\\n\".join(page[\"content\"]) for page in crawled_data[:10]])\n",
    "\n",
    "# Example context and query\n",
    "context = \"Our new product is an AI-powered productivity tool that helps teams collaborate efficiently.\"\n",
    "query = \"How does this product compare to existing solutions like Notion or Trello?\"\n",
    "\n",
    "# Construct final prompt\n",
    "final_prompt = f\"\"\"\n",
    "### Task:\n",
    "You are an AI assistant trained to internalize the reasoning patterns and writing style of a given author based on their blog posts. Your responses should reflect their tone, style, and way of thinking.\n",
    "\n",
    "### Web-Pages Data (Personality Formation):\n",
    "Below are a set of blog posts written by the author. These should be used to understand their reasoning, tone, and perspective.\n",
    "\n",
    "{web_pages_data}\n",
    "\n",
    "### Context (Background Information):\n",
    "This is the necessary background information about our new product. Use it as the factual foundation when answering queries.\n",
    "\n",
    "{context}\n",
    "\n",
    "### Query (User Question):\n",
    "{query}\n",
    "\n",
    "### Expected Response:\n",
    "Using the author's writing style and reasoning pattern, craft a response to the query that aligns with the given product background.\n",
    "\"\"\"\n",
    "\n",
    "print(f'len. final_prompt: {len(final_prompt.split(' '))}')  # You can now feed this prompt into the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len. final_prompt: 572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Error: 500 Server Error: Internal Server Error for url: https://api.atoma.network/v1/chat/completions'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "api_url = \"https://api.atoma.network/v1/chat/completions\"\n",
    "def query_llm(query: str, max_tokens: int = 120000) -> str:\n",
    "        \"\"\"\n",
    "        Send a query to the LLM API and return the response\n",
    "        \"\"\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {os.environ.get('ATOMA_BEARER')}\"\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"stream\": False,\n",
    "            \"model\": \"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "            \"messages\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query\n",
    "            }],\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(api_url, headers=headers, data=json.dumps(payload))\n",
    "            # response = requests.post(self.api_url, headers=headers, json=payload)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse and store the assistant's response\n",
    "            result = response.json()\n",
    "            assistant_message = result['choices'][0]['message']['content']\n",
    "            \n",
    "            # # Add assistant's response to conversation history\n",
    "            # self.conversation_history.append({\n",
    "            #     \"role\": \"assistant\", \n",
    "            #     \"content\": assistant_message\n",
    "            # })\n",
    "            return assistant_message\n",
    "        except requests.RequestException as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "# query_llm('hi')\n",
    "query_llm(final_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "framesmith",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
